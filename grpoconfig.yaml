#model_name: "Qwen/Qwen2.5-Math-1.5B"
model_name: "../outputs/qwen2.5-1.5b-cp75-math/checkpoint-150/"
#model_name: "../outputs/qwen1.5b-math-sft-pro-kk/checkpoint-75/"
#model_name: "../outputs/qwen1.5b-math-sft-pro-kk/checkpoint-1092/"
#model_name: "../outputs/qwen2.5-3b-sft-pro/checkpoint-1092/" 
#model_name: "../outputs/qwen2.5-3b-grpo-humanities-mmlu-small/checkpoint-150/" 
#model_name: "../outputs/qwen2.5-3b-sftkkpro-math-small/checkpoint-125"
output_dir: "../outputs/qwen2.5-1.5b-cp75-math/"
run_name: "qwen1.5b-cp75-math"
# output_dir: "../outputs/qwen2.5-1.5b-sftkk-math/"
# run_name: "qwen1.5b-sftkk-math"
resume_from_checkpoint: True
checkpoint_path: "../outputs/qwen2.5-1.5b-cp75-math/checkpoint-150/"
learning_rate: 1e-6
beta: 0
adam_beta1: 0.9
adam_beta2: 0.99
weight_decay: 0.1
warmup_steps: 25
lr_scheduler_type: constant_with_warmup
logging_steps: 1
bf16: True
bf16_full_eval: True
per_device_train_batch_size: 4
gradient_accumulation_steps: 5
gradient_checkpointing: True
num_generations: 10
max_prompt_length: 256
max_completion_length: 8192 
num_train_epochs: 25
save_steps: 50
max_grad_norm: 0.1
report_to: wandb
use_vllm: True
vllm_max_model_len: 4096
max_steps: -1
log_completions: True
evaluation_strategy: no
eval_steps: 1 
eval_on_start: False 
